---
title: Qué vas a encontrar en este blog
excerpt_separator: "<!--more-->"
categories:
  - General
tags:
  - Introduccion
---

## Consideraciones clave

La `ciencia de datos` es la aplicación de tres tipos de habilidades: de la **computación**, la **estadística** y la **ciencia tradicional** a la resolución de problemas en otros ámbitos tales como la industría, administración, gobierno, etc. En particular el científico de datos utiliza la programación para **automatizar** tareas y capturar información, el método científico para generar preguntas y **modelos** y el conocmiento del área para generar **valor**. Las herramientas típicas incluyen tecnologías de `Big Data`, `Machine Learning` y `Analytics`, que además son buenos boost words para poner en tu curriculum.
{: .notice--danger}

Antes de empezar  

- Los principios generales de la ciencia aplican a la ciencia de datos, aunque con innovaciones
- Los principios de las ciencias sociales aun aplican a la ciencia de datos
- Aun hay que formular preguntas e hipótesis falsables (no todo puede ser automatizado)


Las técnicas computacionales no reemplazan al análisis humano. Para que lo puedas usar correctamente tienes que tener conciancia de su beneficos pero tambiénde sus limitaciones. Aquí una tabla resumen de lo que siempre deves considerar.

|   |  Tradicional |   Computacional |
|-------|--------|---------|
| Pro | - Puede manejar cada caso por separado | - Puede manejar muchos casos  muy rápidamente |
|   | - Puede aplicar cualquier tipo de análisis   |   - El proceso es replicable, sin ningún tipo de error humano |
|   | - Puede detectar tendencias o errores en el proceso  |  - Puede realizar análisis que de otra forma requerirían mucho tiempo o un ejercito de analistas (no factibles) |
|   | - Tiene contexto o información adicional que enriquece el análisis  | - Puede reconocer patrones no detectables por las capacidades humanas |
|   | - Intuición|  - Preciso (Aunque no necesariamente certero) |
|  Contra |   - Sesgo humano | - Procesamiento por lotes de información puede ignorar casos particulares o excepciones (edge cases) | 
|   | - Se cansa (lo que lo hace más proclive a errores) |  - Require hacer presunciones generales sobre los datos, que pueden no ser ciertas |
|   | - Caro (de entrenar a anotadores y analistas) | - Las maquinas tiene sus propios tipos de errores  |
|   | - La infomación contextual puede "contaminar" el análisis|  - Solo puede realizar el análisis para el que fue programado |
|   |   | - Aun puede haber sesgo humano al momeno de seleccionar los datos o durante el proceso (relacionado a las presunciones) |
|   |   |   | 

## Conceptos básicos:

### Teoría de Sistemas:
- Sub-sistemas
- Retroalimentación y estados de equilibrio
- Complejidad, emergencia y caos
 

### Teoría de la Información
- Sources (Entropy, mutual Information, Information gain)
- Message: Coding, compression, loss
- Channels: capacity, rate, noise
- Receptor: Error control and Error detection
 

### Arquitectura Computacional:
- Memory
- CPU
- Storage
- Serial and Parallel computing
 

### Habilidades de programación:
- Data types
- Variables and functions
- Classes and methods
- Flow control (for, while, if, else, case)
- Sistemas (principios de diseño: encapsular, escalabilidad, disponibilidad, resiliencia, consistencia)
 

### Habilidades de Algebra Lineal
- Vector and Matrix manipulation
- Systems of equations

### Habilidades Estadísticas
- Descriptive (mean and median sensibilities to outliers)
- Inference
- Bayesian
- Frequentist: Sample and hypothesis testing
- Regression
 
### Habilidades de Optimización
Frame the problem (error reduction vs increase accuracy, global vs local optimum, objective space)
Close forms and Aproximation solution (Calculus)

## Tutoriales:

### Análisis de series de tiempo:

### Information Retrieval:
- Bibliotecology analysis
- Reverse index Search engine

### Análisis de texto:
- Books finger print or stylometry
 [referencia](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python)
- Word frequency
 [referencia](https://programminghistorian.org/en/lessons/basic-text-processing-in-r)
- Sentiment Analysis
 [referencia](https://programminghistorian.org/en/lessons/sentiment-analysis)
- Topic modelling
    [referencia](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet)
- Embeddings

### Análisis de redes
Clustering communities
[referencia](https://programminghistorian.org/en/lessons/creating-network-diagrams-from-historical-sources)

### Regresión para datos categóricos:
[referencia](https://programminghistorian.org/en/lessons/logistic-regression#fn:2)

### Clasificación
- Supervised image classification

### Clustering

### Análisis de secuencias sociales

### Similaciones sociales
 - Cooperative AI

### Infomación geoespacial
[referencia](https://programminghistorian.org/en/lessons/geocoding-qgis)


## Trucos para manejar el flujo de datos
### Extraer datos:
 - API's [referencia](https://programminghistorian.org/en/lessons/introduction-to-populating-a-website-with-api-data)
 - Scrappers
 - Database repositories
 - SQL

### Limpieza
Las máquinas no son flexibles con los datos, hay que limpiar y estandarizar.
 [referencia](https://programminghistorian.org/en/lessons/fetch-and-parse-data-with-openrefine)

### Transformaciones
 - Summary data
 - Long to wide
 - Wide to long
 - Aggregation
 - Matching
 - Apply, map, reduce and filter

### Visualizaciones
 - Clutering

